{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Titanic machine learning practice exercise! Your job is to build a model to predict whether or not a particular passanger survived the disaster. This is a binary classification problem where the outcome $Y = 1$ if the passanger survived, and $Y = 0$ if not. The goal of this exercise is to get familiar with a typical machine learning model building work-flow, and practice working with data and models.\n",
    "\n",
    "![\"Titanic\"](images/titanic.jpeg)\n",
    "\n",
    "You have the following data about the passengers (some may be missing and you might need to figure out how to guess the missing values)\n",
    "\n",
    "\n",
    "|Variable|\tDefinition\t|Key|\n",
    "| :- |-: | :-: |\n",
    "|survival \t|Survival |\t0 = No, 1 = Yes|\n",
    "|pclass \t|Ticket class| \t1 = 1st, 2 = 2nd, 3 = 3rd|\n",
    "|sex| \tSex \t| |\n",
    "|Age \t|Age in years \t| |\n",
    "|sibsp \t|# of siblings / spouses aboard the Titanic| \t|\n",
    "|parch \t|# of parents / children aboard the Titanic| \t\n",
    "|ticket \t|Ticket number \t| |\n",
    "|fare \t|Passenger fare \t| |\n",
    "|cabin \t|Cabin number| \t    |  \n",
    "|embarked| \tPort of Embarkation| \tC = Cherbourg, Q = Queenstown, S = Southampton|\n",
    "\n",
    "\n",
    "You can build any sort of model you want, but if you are a beginner than you should start with logistic regression, which is a simple yet surprisingly powerful classification model that is important for understanding modern neural network technologies.\n",
    "\n",
    "Logistic regression takes in a set of input data $X \\in \\mathbb{R}^{N_{data} \\times N_{feat}}$ and learns a set of data weights $\\beta \\in \\mathbb{R}^{N_{feat}}$, where \n",
    "$N_{data}, N_{feat}$ are the number of data points and number of predictive features, respectively. You do not have to worry about how the logistic model is trained (at first) for this exercise, because you can use model code from Scikit-learn and simply call the .fit() method. Internally, the model will solve a convex optimization problem that determines $\\beta$ using your data $X$ and your set of outcome lables $Y$.\n",
    "\n",
    "During prediction time, the logistic regression model makes a prediction $\\hat{y_i}$ for a new datapoint (passenger) $x_i$ by the following formula\n",
    "\n",
    "$P(y_i = 1) = \\sigma(x_i \\cdotp \\beta)$\n",
    "\n",
    "where \n",
    " \n",
    "$\\sigma(t) = \\frac{1}{1 + e^{-t}}$.\n",
    " \n",
    "Since these predictions are probabilities, you can turn them into hard predictions by using a threshold of 0.5, that is \n",
    "\n",
    "$\\hat{y_i} = 1 , \\quad \\text{where } \\sigma(x_i \\cdotp \\beta) > 0.5$.\n",
    "\n",
    "To complete this task you will need to\n",
    "- Manipulate the data so that it can inputed into the scikit-learn LogisticRegression class. You will need to recode any categorical variables that you want to use (Why is that?). To keep things simple you can use one-hot encoding (google it!), but be careful to eliminate one category from your one-hot encoding (why this?). What could go wrong if you encode categorical variables with more than two categories as numbers? \n",
    "\n",
    "- Train your model on the training data, using the features which you think are important. You can use penalty=None to train a simple unregularized model. Then make a prediction on the test data. You can submit your results to [Kaggle](https://www.kaggle.com/c/titanic) to get your accuracy score and see how good your model is. Using as many variables as possible can help you to get a good training accuracy, but this doesn't necessarily mean that your model will generalize well to the test-set! Finding a good model usually takes some insight into the data and problem, as well as machine learning skill. You can also use cross-validation with the training data to pick a good model before going to the test set. \n",
    "\n",
    "If you complete these tasks very quickly and would like to go further, you can try the following bonus tasks.\n",
    "\n",
    "- Bonus task 1: Try out some feature engineering. Make a new data column in the training data by using transformations of existing columns. Ratios of columns, log transforms, and power transforms (e.g. $x^2$) are all popular choices that you can play with. Can you improve your test-set classification accuracy by feature engineering?\n",
    "\n",
    "- Bonus task 2: Implement your own logistic regression model using numpy. You can use scipy.optimize to train your model using the method of maximum likelihood. To do this you will need to solve the following optimization problem\n",
    "\n",
    "$\\min_{\\beta} l(\\beta, Y, X)$\n",
    "\n",
    "where the cross-entropy loss function $l$ is given by\n",
    "\n",
    "$l(\\beta, Y, X) = -1 *\\sum_{i =1}^{N_{data}} y_i \\log(\\sigma(x_i\\cdotp \\beta)) + (1 -y_i) \\log(1 - \\sigma(x_i\\cdotp \\beta))$.\n",
    "\n",
    "You can derive this function by taking the log of the likelihood and multiplying by -1\n",
    "\n",
    "$L(\\beta, Y, X) = \\prod_{i =1}^{N_{data}} P(y_i = \\hat{y_i}| \\beta, X)$,\n",
    "\n",
    "where $\\hat{y_i}$ is the predicted outcome, and $y_i$ the true outcome.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>23.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>7.7500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Name     Sex     Fare\n",
       "0                              Braund, Mr. Owen Harris    male   7.2500\n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  71.2833\n",
       "2                               Heikkinen, Miss. Laina  female   7.9250\n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  53.1000\n",
       "4                             Allen, Mr. William Henry    male   8.0500\n",
       "..                                                 ...     ...      ...\n",
       "886                              Montvila, Rev. Juozas    male  13.0000\n",
       "887                       Graham, Miss. Margaret Edith  female  30.0000\n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female  23.4500\n",
       "889                              Behr, Mr. Karl Howell    male  30.0000\n",
       "890                                Dooley, Mr. Patrick    male   7.7500\n",
       "\n",
       "[891 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_train = pd.read_csv(\"data/titanic/train.csv\")\n",
    "titanic_test = pd.read_csv(\"data/titanic/test.csv\")\n",
    "titanic_train[[\"Name\", \"Sex\", \"Fare\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Female</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>29.699118</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Age  Parch  SibSp  Female  Pclass_2  Pclass_3\n",
       "0    22.000000      0      1       0         0         1\n",
       "1    38.000000      0      1       1         0         0\n",
       "2    26.000000      0      0       1         0         1\n",
       "3    35.000000      0      1       1         0         0\n",
       "4    35.000000      0      0       0         0         1\n",
       "..         ...    ...    ...     ...       ...       ...\n",
       "886  27.000000      0      0       0         1         0\n",
       "887  19.000000      0      0       1         0         0\n",
       "888  29.699118      2      1       1         0         1\n",
       "889  26.000000      0      0       0         0         0\n",
       "890  32.000000      0      0       0         0         1\n",
       "\n",
       "[891 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Here I have made some simple choices, but you can make this as complex as you want.\n",
    "#For some competitive solutions check out the \"code\" section of the Kaggle Titanic competition. \n",
    "feature_cols = [\"Sex\", \"Age\", \"Pclass\", \"Parch\", \"SibSp\"]\n",
    "\n",
    "def recode_data(df_raw):\n",
    "    \"\"\"Here I have done the encoding manually so that you can see it. \n",
    "       But for convenience when you build your own models \n",
    "       you can use the sklearn preprocessing tools. \"\"\"\n",
    "\n",
    "    df_encoded = df_raw.copy()\n",
    "    df_encoded[\"Female\"] = (df_encoded[\"Sex\"] == \"female\").astype(int)\n",
    "\n",
    "    df_encoded[\"Pclass_2\"] = (df_encoded[\"Pclass\"] == 2).astype(int)\n",
    "    df_encoded[\"Pclass_3\"] = (df_encoded[\"Pclass\"] == 3).astype(int)\n",
    "    #Pclass = 1 is not encoded as it would introduce a linear dependence. \n",
    "    #Pclass = 1 corresponds to Pclass_2 and Pclass_3 = 0\n",
    "\n",
    "    df_encoded = df_encoded.drop(columns = [\"Sex\", \"Pclass\"])\n",
    "    #Fill in missing ages with the mean age\n",
    "    df_encoded.loc[df_encoded.isnull().any(axis=1), \"Age\"] = df_encoded.loc[~df_encoded.isnull().any(axis=1), \"Age\"].mean()\n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "X_encoded = recode_data(titanic_train[feature_cols])\n",
    "X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy (10-folds) = 0.787\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "Nfolds = 10\n",
    "Y_train = titanic_train[\"Survived\"]\n",
    "\n",
    "clf = LogisticRegression(penalty = \"none\")\n",
    "cv_accuracy = cross_val_score(clf, X_encoded, Y_train, cv = Nfolds)\n",
    "print(\"Mean Accuracy ({}-folds) = {:.3f}\".format(Nfolds, cv_accuracy.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train on the entire training set and make a prediction\n",
    "clf.fit(X_encoded, Y_train)\n",
    "Y_hat = clf.predict(recode_data(titanic_test[feature_cols]))\n",
    "\n",
    "output = pd.DataFrame(titanic_test[\"PassengerId\"])\n",
    "output[\"Survived\"] = Y_hat\n",
    "output.to_csv(\"my_titanic_predictions.csv\",\n",
    "              index = False)\n",
    "\n",
    "#When I submit this result to Kaggle I get an accuracy of 0.75119."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do my predictions match those from SK_learn? True\n"
     ]
    }
   ],
   "source": [
    "#Custom Logistic Regression Model for bonus task #2\n",
    "from scipy.optimize import minimize\n",
    "from functools import partial\n",
    "\n",
    "class MyLogisticRegression(object):\n",
    "    def __init__(self):\n",
    "        self.beta = None\n",
    "        \n",
    "    def fit(self, Y, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy[\"intercept\"] = 1.0\n",
    "        X_copy = X_copy.astype(float)\n",
    "        beta0 = np.zeros(X_copy.shape[1])\n",
    "        \n",
    "        options ={\"iprint\": 2,\n",
    "                  \"gtol\": 1.0e-4,\n",
    "                  \"maxiter\": 100}\n",
    "        \n",
    "        llh_partial = partial(self.llh,\n",
    "                              Y=Y.values,\n",
    "                              X=X_copy)\n",
    "        \n",
    "        nllh_partial = lambda beta: -1*llh_partial(beta)\n",
    "        \n",
    "        sol = minimize(nllh_partial,\n",
    "                       beta0,\n",
    "                       method = \"L-BFGS-B\",\n",
    "                       jac = False,\n",
    "                       options=options)\n",
    "        self.beta = sol.x\n",
    "    \n",
    "    def llh(self, beta, Y, X):\n",
    "        p = self.predict_probability(beta, X)\n",
    "        \n",
    "        #To avoid problems with inf and nan restrict the log transformations\n",
    "        #to where the arguments will be away from 0.\n",
    "        return np.log(Y[Y ==1]*p[Y ==1]).sum() + np.log((1 - Y[Y==0])*(1 -p[Y==0])).sum()\n",
    "        \n",
    "    def predict_probability(self, beta, X):\n",
    "        mu = np.dot(X, beta)\n",
    "        return 1.0/(1 + np.exp(-mu))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy[\"intercept\"] = 1.0\n",
    "        p = self.predict_probability(self.beta, X_copy.values)\n",
    "        return (p > 0.5).astype(int)\n",
    "    \n",
    "mylogit = MyLogisticRegression()\n",
    "mylogit.fit(Y_train, X_encoded)\n",
    "my_Yhat = mylogit.predict(recode_data(titanic_test[feature_cols]))\n",
    "print(\"Do my predictions match those from SK_learn?\", (my_Yhat == Y_hat).all())\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
